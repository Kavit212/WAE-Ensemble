{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "WAE-Ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM5famhBHFcd2KHqge0RB+7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kavit212/WAE-Ensemble/blob/main/WAE_Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJlr-aXcdteA"
      },
      "source": [
        "!pip install tensorflow-estimator==2.2.*\n",
        "!pip install tensorflow==2.2.0\n",
        "!pip install keras==2.3.1\n",
        "!pip install git+https://github.com/qubvel/segmentation_models\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yAxSJTAxynz"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLs6palydu8h"
      },
      "source": [
        "# Connect to your own data source\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUL2CVRotDJO"
      },
      "source": [
        "%cd /content\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8p3HOQvd1uq"
      },
      "source": [
        "!mkdir 128_patches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxW5DYV_d35v"
      },
      "source": [
        "#change file path according to your data source\n",
        "\n",
        "!tar -xvf \"/content/gdrive/MyDrive/Testing data/J558/J558L_train_images.tar\" -C \"/content/128_patches\"\n",
        "!tar -xvf \"/content/gdrive/MyDrive/Testing data/J558/J558_train_masks.tar\" -C \"/content/128_patches\"\n",
        "\n",
        "\n",
        "!tar -xvf \"/content/gdrive/MyDrive/Testing data/J558/J558L_test_images.tar\" -C \"/content/128_patches\"\n",
        "!tar -xvf \"/content/gdrive/MyDrive/Testing data/J558/J558_test_masks.tar\" -C \"/content/128_patches\"\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn5VsDY5OUZt"
      },
      "source": [
        "!pip install -U git+https://github.com/albu/albumentations --no-cache-dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdcK1btDOb8v"
      },
      "source": [
        "### Image Augmentation\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage.transform import AffineTransform, warp\n",
        "from skimage import io, img_as_ubyte\n",
        "import random\n",
        "import os\n",
        "from scipy.ndimage import rotate\n",
        "\n",
        "import albumentations as A\n",
        "images_to_generate=500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maTA2WKU9QAN"
      },
      "source": [
        "!rm -rf aug_img\n",
        "!rm -rf aug_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4RbCIrnOeGF"
      },
      "source": [
        "!mkdir aug_img\n",
        "!mkdir aug_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltOqk9UbOj35"
      },
      "source": [
        "# change to your own preferred data path\n",
        "\n",
        "images_path=\"/content/128_patches/J558L_train_images\" #path to original images\n",
        "masks_path = \"/content/128_patches/J558_train_masks\"\n",
        "img_augmented_path=\"/content/aug_img\" # path to store aumented images\n",
        "msk_augmented_path=\"/content/aug_mask\" # path to store aumented images\n",
        "images=[] # to store paths of images from folder\n",
        "masks=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm7Jk8adOqxu"
      },
      "source": [
        "for im in os.listdir(images_path):  # read image name from folder and append its path into \"images\" array     \n",
        "    images.append(os.path.join(images_path,im))\n",
        "\n",
        "for msk in os.listdir(masks_path):  # read image name from folder and append its path into \"images\" array     \n",
        "    masks.append(os.path.join(masks_path,msk))\n",
        "\n",
        "\n",
        "aug = A.Compose([\n",
        "    A.VerticalFlip(p=1),              \n",
        "    A.RandomRotate90(p=1),\n",
        "    A.HorizontalFlip(p=1),\n",
        "    A.Transpose(p=1),\n",
        "    #A.ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
        "    A.GridDistortion(p=1)\n",
        "    ]\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9qlFvB-Ou3K"
      },
      "source": [
        "random.seed(42)\n",
        "\n",
        "i=1   # variable to iterate till images_to_generate\n",
        "\n",
        "\n",
        "while i<=images_to_generate: \n",
        "    number = random.randint(0, len(images)-1)  #PIck a number to select an image & mask\n",
        "    image = images[number]\n",
        "    mask = masks[number]\n",
        "    #print(image, mask)\n",
        "    #image=random.choice(images) #Randomly select an image name\n",
        "    original_image = io.imread(image)\n",
        "    original_mask = io.imread(mask)\n",
        "    \n",
        "    augmented = aug(image=original_image, mask=original_mask)\n",
        "    transformed_image = augmented['image']\n",
        "    transformed_mask = augmented['mask']\n",
        "\n",
        "        \n",
        "    new_image_path= \"%s/augmented_image_%s.png\" %(img_augmented_path, i)\n",
        "    new_mask_path = \"%s/augmented_image_%s.png\" %(msk_augmented_path, i)\n",
        "    io.imsave(new_image_path, transformed_image)\n",
        "    io.imsave(new_mask_path, transformed_mask)\n",
        "    i =i+1\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ON9I-r5uO19A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDwqzG5ud6NH"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "%env SM_FRAMEWORK=tf.keras\n",
        "\n",
        "import segmentation_models as sm\n",
        "\n",
        "import glob\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "#import keras \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#from tf.keras.utils import normalize \n",
        "\n",
        "\n",
        "#from tf.keras.metrics import MeanIoU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8FhUZtnObH6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCa5XPTld633"
      },
      "source": [
        "#Resizing images, if needed\n",
        "SIZE_X = 512\n",
        "SIZE_Y = 512\n",
        "#Number of classes for segmentation\n",
        "n_classes = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMpuygyceAyf"
      },
      "source": [
        "#Capture training image info as a list\n",
        "train_images = []\n",
        "\n",
        "for directory_path in glob.glob(\"/content/aug_img\"):\n",
        "    for img_path in glob.glob(os.path.join(directory_path, \"*.png\")):\n",
        "        img = cv2.imread(img_path, 1)       \n",
        "        img = cv2.resize(img, (SIZE_Y, SIZE_X))\n",
        "        train_images.append(img)\n",
        "       \n",
        "#Convert list to array for machine learning processing        \n",
        "train_images = np.array(train_images)\n",
        "\n",
        "#Capture mask/label info as a list\n",
        "train_masks = [] \n",
        "for directory_path in glob.glob(\"/content/aug_mask\"):\n",
        "    for mask_path in glob.glob(os.path.join(directory_path, \"*.png\")):\n",
        "        mask = cv2.imread(mask_path, 0)       \n",
        "        mask = cv2.resize(mask, (SIZE_Y, SIZE_X), interpolation = cv2.INTER_NEAREST)  #Otherwise ground truth changes due to interpolation\n",
        "        train_masks.append(mask)\n",
        "        \n",
        "#Convert list to array for machine learning processing          \n",
        "train_masks = np.array(train_masks)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7m8nCHGeC5t"
      },
      "source": [
        "print(train_images.shape)\n",
        "print(train_masks.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AYZKH8BioCw"
      },
      "source": [
        "#Capture test image info as a list\n",
        "test_images = []\n",
        "\n",
        "for directory_path in glob.glob(\"/content/128_patches/J558L_test_images\"):\n",
        "    for img_path in glob.glob(os.path.join(directory_path, \"*.png\")):\n",
        "        img = cv2.imread(img_path, 1)       \n",
        "        #img = cv2.resize(img, (SIZE_Y, SIZE_X))\n",
        "        test_images.append(img)\n",
        "       \n",
        "#Convert list to array for machine learning processing        \n",
        "test_images = np.array(test_images)\n",
        "\n",
        "#Capture mask/label info as a list\n",
        "test_masks = [] \n",
        "for directory_path in glob.glob(\"/content/128_patches/J558_test_masks\"):\n",
        "    for mask_path in glob.glob(os.path.join(directory_path, \"*.png\")):\n",
        "        mask = cv2.imread(mask_path, 0)       \n",
        "        #mask = cv2.resize(mask, (SIZE_Y, SIZE_X), interpolation = cv2.INTER_NEAREST)  #Otherwise ground truth changes due to interpolation\n",
        "        test_masks.append(mask)\n",
        "        \n",
        "#Convert list to array for machine learning processing          \n",
        "test_masks = np.array(test_masks)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFG35_qPjTRp"
      },
      "source": [
        "print(test_images.shape)\n",
        "print(test_masks.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srZ4RHZjeFEo"
      },
      "source": [
        "###############################################\n",
        "#Encode labels... but multi dim array so need to flatten, encode and reshape\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder = LabelEncoder()\n",
        "n, h, w = train_masks.shape\n",
        "train_masks_reshaped = train_masks.reshape(-1,1)\n",
        "train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\n",
        "train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
        "\n",
        "np.unique(train_masks_encoded_original_shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1xTYos3jeG1"
      },
      "source": [
        "###############################################\n",
        "#Encode labels... but multi dim array so need to flatten, encode and reshape\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder = LabelEncoder()\n",
        "n, h, w = test_masks.shape\n",
        "test_masks_reshaped = test_masks.reshape(-1,1)\n",
        "test_masks_reshaped_encoded = labelencoder.fit_transform(test_masks_reshaped)\n",
        "test_masks_encoded_original_shape = test_masks_reshaped_encoded.reshape(n, h, w)\n",
        "\n",
        "np.unique(test_masks_encoded_original_shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJZLGOWWjygJ"
      },
      "source": [
        "#train_images = np.expand_dims(train_images, axis=3)\n",
        "#train_images = normalize(train_images, axis=1)\n",
        "\n",
        "#test_images = np.expand_dims(test_images, axis=3)\n",
        "#test_images = normalize(test_images, axis=1)\n",
        "\n",
        "train_masks_input = np.expand_dims(train_masks_encoded_original_shape, axis=3)\n",
        "test_masks_input = np.expand_dims(test_masks_encoded_original_shape, axis=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymGmH7cbeJOZ"
      },
      "source": [
        "#################################################\n",
        "#train_images = np.expand_dims(train_images, axis=3)\n",
        "#train_images = normalize(train_images, axis=1)\n",
        "\n",
        "#train_masks_input = np.expand_dims(train_masks_encoded_original_shape, axis=3)\n",
        "\n",
        "\n",
        "\n",
        "X_train = train_images\n",
        "y_train = train_masks_input\n",
        "X_test = test_images\n",
        "y_test = test_masks_input\n",
        "\n",
        "\n",
        "print(\"Class values in the dataset are ... \", np.unique(y_train))  # 0 is the background/few unlabeled \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVtEF7Syt8bG"
      },
      "source": [
        "#Sanity check, view few mages\n",
        "import random\n",
        "import numpy as np\n",
        "image_number = random.randint(0, len(X_train))\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(121)\n",
        "plt.imshow(X_train[image_number, :,:, 0], cmap='gray')\n",
        "plt.subplot(122)\n",
        "plt.imshow(np.reshape(y_train[image_number], (512,512)), cmap='gray')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2oPR-tQfK3i"
      },
      "source": [
        "\n",
        "#from keras.utils import to_categorical\n",
        "train_masks_cat = tf.keras.utils.to_categorical(y_train, num_classes=n_classes)\n",
        "y_train_cat = train_masks_cat.reshape((y_train.shape[0], y_train.shape[1], y_train.shape[2], n_classes))\n",
        "\n",
        "\n",
        "test_masks_cat = tf.keras.utils.to_categorical(y_test, num_classes=n_classes)\n",
        "y_test_cat = test_masks_cat.reshape((y_test.shape[0], y_test.shape[1], y_test.shape[2], n_classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynxavBlKeLQD"
      },
      "source": [
        "#Reused parameters in all models\n",
        "\n",
        "n_classes=3\n",
        "activation='softmax'\n",
        "\n",
        "LR = 0.0001\n",
        "optim = keras.optimizers.Adam(LR)\n",
        "\n",
        "\n",
        "# set class weights for dice_loss according to number of classes \n",
        "#dice_loss = sm.losses.DiceLoss(class_weights=np.array([0.25, 0.25, 0.25, 0.25]))\n",
        "dice_loss = sm.losses.DiceLoss(class_weights=np.array([0.33, 0.33, 0.33]))  \n",
        "#dice_loss = sm.losses.DiceLoss(class_weights=np.array([0.2, 0.2, 0.2, 0.2, 0.2]))\n",
        "#dice_loss = sm.losses.DiceLoss()\n",
        "focal_loss = sm.losses.CategoricalFocalLoss()\n",
        "total_loss = dice_loss + (1 * focal_loss)\n",
        "\n",
        "\n",
        "\n",
        "metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF7YZ_Jeiktv"
      },
      "source": [
        "################\n",
        "# Calbacks\n",
        "\n",
        "\n",
        "\n",
        "csv_logger1 = tf.keras.callbacks.CSVLogger('/content/gdrive/MyDrive/Test_Output/Ensemble_Wanner/log_Ensemble_Resnet34-N.csv', append=False, separator=',')\n",
        "csv_logger2 = tf.keras.callbacks.CSVLogger('/content/gdrive/MyDrive/Test_Output/Ensemble_Wanner/log_Ensemble_Inception-N.csv', append=False, separator=',')\n",
        "csv_logger3 = tf.keras.callbacks.CSVLogger('/content/gdrive/MyDrive/Test_Output/Ensemble_Wanner/log_Ensemble_Vgg19-N.csv', append=False, separator=',')\n",
        "csv_logger4 = tf.keras.callbacks.CSVLogger('/content/gdrive/MyDrive/Test_Output/Ensemble_Wanner/log_Ensemble_Seresnet-N.csv', append=False, separator=',')\n",
        "csv_logger5 = tf.keras.callbacks.CSVLogger('/content/gdrive/MyDrive/Test_Output/Ensemble_Wanner/log_Ensemble_EfficientnetB4-N.csv', append=False, separator=',')\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=10, mode='min', verbose=0)\n",
        "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, verbose=0, mode='min')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QpMPOmPeNwj"
      },
      "source": [
        "########################################################################\n",
        "###Model 1\n",
        "BACKBONE1 = 'resnet34'\n",
        "\n",
        "preprocess_input1 = sm.get_preprocessing(BACKBONE1)\n",
        "\n",
        "# preprocess input\n",
        "X_train1 = preprocess_input1(X_train)\n",
        "X_test1 = preprocess_input1(X_test)\n",
        "\n",
        "# define model\n",
        "model1 = sm.Unet(BACKBONE1, encoder_weights='imagenet', classes=n_classes, activation=activation)\n",
        "\n",
        "# compile keras model with defined optimozer, loss and metrics\n",
        "model1.compile(optim, total_loss, metrics=metrics)\n",
        "\n",
        "\n",
        "#print(model1.summary())\n",
        "\n",
        "\n",
        "history1=model1.fit(X_train1, \n",
        "          y_train_cat,\n",
        "          batch_size=8, \n",
        "          epochs=500,\n",
        "          verbose=1,\n",
        "          callbacks=[earlyStopping, reduce_lr, csv_logger1],\n",
        "          validation_data=(X_test1, y_test_cat))\n",
        "\n",
        "\n",
        "model1.save('/content/gdrive/MyDrive/Test_Output/Ensemble_Wanner/resnet34-N.hdf5')\n",
        "#model1.save('platelet_resnet.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8MavUQwmEgo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fXeYBf0eQMN"
      },
      "source": [
        "############################################################\n",
        "###Model 2\n",
        "\n",
        "BACKBONE2 = 'inceptionv3'\n",
        "preprocess_input2 = sm.get_preprocessing(BACKBONE2)\n",
        "\n",
        "# preprocess input\n",
        "X_train2 = preprocess_input2(X_train)\n",
        "X_test2 = preprocess_input2(X_test)\n",
        "\n",
        "# define model\n",
        "model2 = sm.Unet(BACKBONE2, encoder_weights='imagenet', classes=n_classes, activation=activation)\n",
        "\n",
        "\n",
        "# compile keras model with defined optimozer, loss and metrics\n",
        "model2.compile(optim, total_loss, metrics)\n",
        "\n",
        "\n",
        "\n",
        "#print(model2.summary())\n",
        "\n",
        "\n",
        "history2=model2.fit(X_train2, \n",
        "          y_train_cat,\n",
        "          batch_size=8, \n",
        "          epochs=500,\n",
        "          verbose=1,\n",
        "          callbacks=[reduce_lr, earlyStopping, csv_logger2],\n",
        "          validation_data=(X_test2, y_test_cat))\n",
        "\n",
        "\n",
        "model2.save('/content/gdrive/MyDrive/Test_Output/Ensemble_Wanner/INception-N.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZVHs5nNmwae"
      },
      "source": [
        "#####################################################\n",
        "###Model 3\n",
        "\n",
        "BACKBONE3 = 'vgg19'\n",
        "#BACKBONE3 = 'seresnet34'\n",
        "preprocess_input3 = sm.get_preprocessing(BACKBONE3)\n",
        "\n",
        "# preprocess input\n",
        "X_train3 = preprocess_input3(X_train)\n",
        "X_test3 = preprocess_input3(X_test)\n",
        "\n",
        "\n",
        "# define model\n",
        "model3 = sm.Unet(BACKBONE3, encoder_weights='imagenet', classes=n_classes, activation=activation)\n",
        "\n",
        "# compile keras model with defined optimozer, loss and metrics\n",
        "model3.compile(optim, total_loss, metrics)\n",
        "\n",
        "\n",
        "\n",
        "#print(model3.summary())\n",
        "\n",
        "history3=model3.fit(X_train3, \n",
        "          y_train_cat,\n",
        "          batch_size=8, \n",
        "          epochs=500,\n",
        "          verbose=1,\n",
        "          callbacks=[csv_logger3, reduce_lr, earlyStopping],\n",
        "          validation_data=(X_test3, y_test_cat))\n",
        "\n",
        "\n",
        "model3.save('/content/gdrive/MyDrive/Test_Output/Ensemble_Wanner/VGG19-N.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2GLLeBL851x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yODulDjeVn5"
      },
      "source": [
        "#####################################################\n",
        "###Model 4\n",
        "\n",
        "BACKBONE4 = 'seresnet34'\n",
        "preprocess_input4 = sm.get_preprocessing(BACKBONE4)\n",
        "\n",
        "# preprocess input\n",
        "X_train4 = preprocess_input4(X_train)\n",
        "X_test4 = preprocess_input4(X_test)\n",
        "\n",
        "\n",
        "# define model\n",
        "model4 = sm.Unet(BACKBONE4, encoder_weights='imagenet', classes=n_classes, activation=activation)\n",
        "\n",
        "# compile keras model with defined optimozer, loss and metrics\n",
        "model4.compile(optim, total_loss, metrics)\n",
        "\n",
        "\n",
        "\n",
        "#print(model3.summary())\n",
        "\n",
        "history4=model4.fit(X_train4, \n",
        "          y_train_cat,\n",
        "          batch_size=1, \n",
        "          epochs=500,\n",
        "          verbose=1,\n",
        "          callbacks=[csv_logger4, reduce_lr, earlyStopping],\n",
        "          validation_data=(X_test4, y_test_cat))\n",
        "\n",
        "\n",
        "model4.save('/content/gdrive/MyDrive/Test_Output/Ensemble_Wanner/Seresnet-N.hdf5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYqySLWWq_1H"
      },
      "source": [
        "#####################################################\n",
        "###Model 5\n",
        "\n",
        "BACKBONE5 = 'efficientnetb4'\n",
        "\n",
        "preprocess_input5 = sm.get_preprocessing(BACKBONE5)\n",
        "\n",
        "# preprocess input\n",
        "X_train5 = preprocess_input5(X_train)\n",
        "X_test5 = preprocess_input5(X_test)\n",
        "\n",
        "\n",
        "# define model\n",
        "model5 = sm.Unet(BACKBONE5, encoder_weights='imagenet', classes=n_classes, activation=activation)\n",
        "\n",
        "# compile keras model with defined optimozer, loss and metrics\n",
        "model5.compile(optim, total_loss, metrics)\n",
        "\n",
        "\n",
        "\n",
        "#print(model3.summary())\n",
        "\n",
        "history5=model5.fit(X_train5, \n",
        "          y_train_cat,\n",
        "          batch_size=1, \n",
        "          epochs=500,\n",
        "          verbose=1,\n",
        "          callbacks=[csv_logger5, reduce_lr, earlyStopping],\n",
        "          validation_data=(X_test5, y_test_cat))\n",
        "\n",
        "\n",
        "model5.save('/content/gdrive/MyDrive/Test_Output/Ensemble_Wanner/EfficiennetB4-N.hdf5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4sDIBQ7eYRn"
      },
      "source": [
        "\n",
        "#plot the training and validation accuracy and loss at each epoch\n",
        "loss = history1.history['loss']\n",
        "val_loss = history1.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "acc = history1.history['iou_score']\n",
        "val_acc = history1.history['val_iou_score']\n",
        "\n",
        "plt.plot(epochs, acc, 'y', label='Training IOU')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation IOU')\n",
        "plt.title('Training and validation IOU')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('IOU')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ptq9LyBtrjG"
      },
      "source": [
        "#####Loading previously trained models ###################\n",
        "####Can be expanded to load 5 models at the same time ################\n",
        "##### load best three performing models##############\n",
        "\n",
        "###Model 1\n",
        "BACKBONE1 = 'resnet34'\n",
        "\n",
        "\n",
        "preprocess_input1 = sm.get_preprocessing(BACKBONE1)\n",
        "\n",
        "# preprocess input\n",
        "X_train1 = preprocess_input1(X_train)\n",
        "X_test1 = preprocess_input1(X_test)\n",
        "\n",
        "############################################################\n",
        "###Model 2\n",
        "\n",
        "BACKBONE2 = 'inceptionv3'\n",
        "\n",
        "preprocess_input2 = sm.get_preprocessing(BACKBONE2)\n",
        "\n",
        "# preprocess input\n",
        "X_train2 = preprocess_input2(X_train)\n",
        "X_test2 = preprocess_input2(X_test)\n",
        "\n",
        "#####################################################\n",
        "###Model 3\n",
        "\n",
        "BACKBONE3 = 'vgg19'\n",
        "\n",
        "preprocess_input3 = sm.get_preprocessing(BACKBONE3)\n",
        "\n",
        "# preprocess input\n",
        "X_train3 = preprocess_input3(X_train)\n",
        "X_test3 = preprocess_input3(X_test)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbdgTHNsYV2L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3ICcE2sebHd"
      },
      "source": [
        "#from keras.models import load_model\n",
        "\n",
        "\n",
        "model1 = tf.keras.models.load_model('/content/gdrive/MyDrive/Test_Output/Ensemble_PrHu/resnet34-AUG.hdf5', compile=False)\n",
        "model2 = tf.keras.models.load_model('/content/gdrive/MyDrive/Test_Output/Ensemble_PrHu/INception-AUG.hdf5', compile=False)\n",
        "model3 = tf.keras.models.load_model('/content/gdrive/MyDrive/Test_Output/Ensemble_PrHu/VGG19-AUG.hdf5', compile=False)\n",
        "#model3 = tf.keras.models.load_model('/content/gdrive/MyDrive/saved_models_20/EfficientnetB4_BONcell6_6k_Aug.hdf5', compile=False)\n",
        "\n",
        "#Weighted average ensemble\n",
        "models = [model1, model2, model3]\n",
        "#preds = [model.predict(X_test) for model in models]\n",
        "\n",
        "pred1 = model1.predict(X_test1)\n",
        "pred2 = model2.predict(X_test2)\n",
        "pred3 = model3.predict(X_test3)\n",
        "\n",
        "preds=np.array([pred1, pred2, pred3])\n",
        "\n",
        "#preds=np.array(preds)\n",
        "weights = [0.4, 0.4, 0.2]\n",
        "#weights = [0.15, 0.7, 0.15]\n",
        "\n",
        "#Use tensordot to sum the products of all elements over specified axes.\n",
        "weighted_preds = np.tensordot(preds, weights, axes=((0),(0)))\n",
        "weighted_ensemble_prediction = np.argmax(weighted_preds, axis=3)\n",
        "\n",
        "y_pred1_argmax=np.argmax(pred1, axis=3)\n",
        "y_pred2_argmax=np.argmax(pred2, axis=3)\n",
        "y_pred3_argmax=np.argmax(pred3, axis=3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYVOcRRxebk2"
      },
      "source": [
        "# individual model prediction\n",
        "n_classes = 3\n",
        "IOU1 = tf.keras.metrics.MeanIoU(num_classes=n_classes)  \n",
        "IOU2 = tf.keras.metrics.MeanIoU(num_classes=n_classes)  \n",
        "IOU3 = tf.keras.metrics.MeanIoU(num_classes=n_classes) \n",
        "IOU_weighted = tf.keras.metrics.MeanIoU(num_classes=n_classes)  \n",
        "\n",
        "IOU1.update_state(y_test[:,:,:,0], y_pred1_argmax)\n",
        "IOU2.update_state(y_test[:,:,:,0], y_pred2_argmax)\n",
        "IOU3.update_state(y_test[:,:,:,0], y_pred3_argmax)\n",
        "IOU_weighted.update_state(y_test[:,:,:,0], weighted_ensemble_prediction)\n",
        "\n",
        "\n",
        "print('IOU Score for model1 = ', IOU1.result().numpy())\n",
        "print('IOU Score for model2 = ', IOU2.result().numpy())\n",
        "print('IOU Score for model3 = ', IOU3.result().numpy())\n",
        "print('IOU Score for weighted average ensemble = ', IOU_weighted.result().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx3CPpJC9TSJ"
      },
      "source": [
        "#Individual class prediction\n",
        "values = np.array(IOU_weighted.get_weights()).reshape(n_classes, n_classes)\n",
        "print(values)\n",
        "class1_IoU = values[0,0]/(values[0,0] + values[0,1] + values[0,2] + values[1,0]+ values[2,0])\n",
        "class2_IoU = values[1,1]/(values[1,1] + values[1,0] + values[1,2] + values[0,1]+ values[2,1])\n",
        "class3_IoU = values[2,2]/(values[2,2] + values[2,0] + values[2,1] + values[0,2]+ values[1,2])\n",
        "\n",
        "\n",
        "print(\"IoU for class1 is: \", class1_IoU)\n",
        "print(\"IoU for class2 is: \", class2_IoU)\n",
        "print(\"IoU for class3 is: \", class3_IoU)\n",
        "\n",
        "\n",
        "plt.imshow(train_images[0, :,:,0], cmap='gray')\n",
        "plt.imshow(train_masks[0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwyuiMfFeeBW"
      },
      "source": [
        "###########################################\n",
        "#Grid search for the best combination of w1, w2, w3 that gives maximum acuracy\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame([])\n",
        "\n",
        "for w1 in range(0,3):\n",
        "    for w2 in range(0,3):\n",
        "        for w3 in range(0,3):\n",
        "            wts = [w1/10.,w2/10.,w3/10]\n",
        "            \n",
        "            IOU_wted = tf.keras.metrics.MeanIoU(num_classes=n_classes) \n",
        "            wted_preds = np.tensordot(preds, wts, axes=((0),(0)))\n",
        "            wted_ensemble_pred = np.argmax(wted_preds, axis=3)\n",
        "            IOU_wted.update_state(y_test[:,:,:,0], wted_ensemble_pred)\n",
        "            print(\"Now predciting for weights :\", w1/10., w2/10., w3/10., \" : IOU = \", IOU_wted.result().numpy())\n",
        "            df = df.append(pd.DataFrame({'wt1':wts[0],'wt2':wts[1], \n",
        "                                         'wt3':wts[2], 'IOU': IOU_wted.result().numpy()}, index=[0]), ignore_index=True)\n",
        "            \n",
        "max_iou_row = df.iloc[df['IOU'].idxmax()]\n",
        "print(\"Max IOU of \", max_iou_row[3], \" obained with w1=\", max_iou_row[0],\n",
        "      \" w2=\", max_iou_row[1], \" and w3=\", max_iou_row[2])         \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3jphDM9ejhy"
      },
      "source": [
        "#############################################################\n",
        "opt_weights = [max_iou_row[0], max_iou_row[1], max_iou_row[2]]\n",
        "\n",
        "#Use tensordot to sum the products of all elements over specified axes.\n",
        "opt_weighted_preds = np.tensordot(preds, opt_weights, axes=((0),(0)))\n",
        "opt_weighted_ensemble_prediction = np.argmax(opt_weighted_preds, axis=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5CmNmZCemFV"
      },
      "source": [
        "#######################################################\n",
        "#Predict on a few images\n",
        "\n",
        "#import random\n",
        "#test_img_number = random.randint(0, len(X_test))\n",
        "#test_img = X_test[test_img_number]\n",
        "test_img_number = 10\n",
        "test_img = X_test[test_img_number]\n",
        "ground_truth=y_test[test_img_number]\n",
        "test_img_norm=test_img[:,:,:]\n",
        "test_img_input=np.expand_dims(test_img_norm, 0)\n",
        "\n",
        "#Weighted average ensemble\n",
        "models = [model1, model2, model3]\n",
        "\n",
        "test_img_input1 = preprocess_input1(test_img_input)\n",
        "test_img_input2 = preprocess_input2(test_img_input)\n",
        "test_img_input3 = preprocess_input3(test_img_input)\n",
        "\n",
        "test_pred1 = model1.predict(test_img_input1)\n",
        "test_pred2 = model2.predict(test_img_input2)\n",
        "test_pred3 = model3.predict(test_img_input3)\n",
        "\n",
        "test_preds=np.array([test_pred1, test_pred2, test_pred3])\n",
        "\n",
        "#Use tensordot to sum the products of all elements over specified axes.\n",
        "weighted_test_preds = np.tensordot(test_preds, opt_weights, axes=((0),(0)))\n",
        "weighted_ensemble_test_prediction = np.argmax(weighted_test_preds, axis=3)[0,:,:]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(231)\n",
        "plt.title('Testing Image')\n",
        "plt.imshow(test_img[:,:,0], cmap='gray')\n",
        "\n",
        "plt.subplot(232)\n",
        "plt.title('Testing Label')\n",
        "plt.imshow(ground_truth[:,:,0], cmap='jet')\n",
        "\n",
        "plt.subplot(233)\n",
        "plt.title('Prediction on test image')\n",
        "plt.imshow(weighted_ensemble_test_prediction, cmap='jet')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#####################################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9dU8_GN82ip"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbQjJxGAQMPG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}